{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8404c0c2",
   "metadata": {},
   "source": [
    "<center><h1>Introduction to Reinforcement Learning</h1></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911aba41",
   "metadata": {},
   "source": [
    "<p>\n",
    " In this tutorial, we will be discussing the basics of Reinforcement Learning and how to implement a simple RL model using the DQN (Deep Q-Network) algorithm.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70b8a77",
   "metadata": {},
   "source": [
    "# I) What is Reinforcement Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a1303d",
   "metadata": {},
   "source": [
    "Reinforcement Learning is a machine learning method that depends on feedback processes. In this approach, an agent learns how to interact with its environment by carrying out actions and assessing the resulting consequences. When the agent makes a favorable choice, it receives positive feedback; on the other hand, negative feedback or penalties are given for detrimental decisions.\n",
    "\n",
    "The main components of a Reinforcement Learning system are:\n",
    "\n",
    "- Agent: The learning entity that makes decisions and takes actions.\n",
    "- Environment: The world in which the agent interacts and takes actions.\n",
    "- State: A snapshot of the current situation in the environment.\n",
    "- Action: A decision made by the agent that affects the environment.\n",
    "- Reward: Feedback given to the agent based on the outcome of an action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ab0be2",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <figure>\n",
    "    <img src=\"Agent-RL-Illustration.png\" alt=\"Agent RL Illustration\" width=\"500\">\n",
    "    <figcaption>Source: <a href=\"https://vitalflux.com/reinforcement-learning-real-world-examples/?utm_content=cmp-true\">vitalflux</a></figcaption>\n",
    "  </figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e878f1e3",
   "metadata": {},
   "source": [
    "# II) Basic Concepts in RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f28feac",
   "metadata": {},
   "source": [
    "### a) Q-Values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62598db",
   "metadata": {},
   "source": [
    "<p>Q-values represent the expected cumulative reward an agent can obtain by performing an action in a given state. In Q-learning, we use a Q-table or Q-function to estimate these values for each state-action pair. The goal is to learn the optimal Q-function, which can be used to determine the best action in each state.</p>\n",
    "\n",
    "<p>The Q-learning algorithm updates the Q-values iteratively using the following update rule:</p>\n",
    "\n",
    "<pre>\n",
    "Q(s, a) = Q(s, a) + α * (r + γ * max<sub>a'</sub> Q(s', a') - Q(s, a))\n",
    "</pre>\n",
    "\n",
    "<p>Here, <b>s</b> represents the current state, <b>a</b> is the action taken, <b>r</b> is the immediate reward received, <b>s'</b> is the next state, <b>a'</b> is the next action, <b>α</b> is the learning rate, and <b>γ</b> is the discount factor.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc29475",
   "metadata": {},
   "source": [
    "### b) Value function\n",
    "The value function, denoted as V(s), represents the expected cumulative reward an agent can obtain starting from a given state and following a specific policy. The value function is related to the Q-values, as the value of a state is the maximum Q-value for that state:\n",
    "\n",
    "<b> V(s) = max_a Q(s, a)</b>\n",
    "\n",
    "### c) Policy function\n",
    "\n",
    "The policy function, denoted as π(s), represents the action the agent should take in a given state to maximize its expected cumulative reward. The policy function is derived from the Q-values, as it selects the action with the highest Q-value for each state:\n",
    "\n",
    "<b>π(s) = argmax_a Q(s, a)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a7787f",
   "metadata": {},
   "source": [
    "# III) Deep Q-Networks (DQN) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8360b6",
   "metadata": {},
   "source": [
    "In many practical problems, the state and action spaces are too large to represent the Q-values in a table. In such cases, we can use a neural network, called a Deep Q-Network (DQN), to approximate the Q-function. A DQN takes the state as input and produces Q-values for each action as output.\n",
    "\n",
    "##### Experience Replay\n",
    "\n",
    "- Experience replay is a technique used to improve the stability and efficiency of the DQN. Instead of updating the network with consecutive samples, the agent stores the experiences (state, action, reward, next state, done) in a memory buffer and samples a mini-batch of experiences to update the network. This helps to break the correlation between samples and improves the learning process.\n",
    "\n",
    "##### Target Networks\n",
    "\n",
    "- Another technique used in DQN is the use of a separate target network to estimate the Q-values for the next state during the update step. This network has the same architecture as the original DQN but with separate parameters. The target network's parameters are periodically updated with the main DQN's parameters to provide more stable Q-value estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e4c257",
   "metadata": {},
   "source": [
    "# IV) First RL Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f4c396",
   "metadata": {},
   "source": [
    "In this part of the tutorial, we will implement a simple reinforcement learning model using the concepts discussed above. The model consists of a custom game environment, a DQN to estimate Q-values, and an agent that uses the DQN to navigate through the environment.\n",
    "\n",
    "###  a) Game Environment Class\n",
    "\n",
    "\n",
    "Imagine a grid-based labyrinth where an agent must find its way out. The agent can take 4 actions: up, down, left, and right. The grid has different types of cells, including:\n",
    "\n",
    "- Empty cells: The agent can move freely in these cells.\n",
    "- Walls(obstacle 1): If the agent tries to move into a wall, it will receive a negative reward (-1) and stay in its current cell.\n",
    "- Holes (obstacle 2): If the agent falls into a hole, it dies and receives a negative reward (-100).\n",
    "- Magic doors (obstacle 3): When the agent encounters a magic door, it gets teleported closer to the exit and receives a +20 reward.\n",
    "- If the agent does not find the exit after 300 steps, it dies and receives a negative reward.\n",
    "- If the agent does find the exit, it gets a postive reward (+100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4205faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import os\n",
    "\n",
    "class GameEnv:\n",
    "    def __init__(self):\n",
    "        self.board_size = 10\n",
    "        self.board = np.zeros((self.board_size, self.board_size), dtype=int)\n",
    "        #Obstacle de niveau 1, mur\n",
    "        self.obstacles_1 = [(4, 8), (4, 2), (7, 3), (3, 5), (8, 0)]\n",
    "        self.obstacles_2 = [(6, 6), (3, 4), (4, 4), (7, 7)]\n",
    "        self.obstacles_3 = [(4, 9), (3, 2)]\n",
    "\n",
    "        self.numbre_de_tour = 0\n",
    "\n",
    "        self.max_steps = 300\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.board.fill(0)\n",
    "        self.p1_pos = (0, 0)\n",
    "        \n",
    "        self.board[self.p1_pos] = 1\n",
    "        for obs in self.obstacles_1:\n",
    "            self.board[obs] = -1\n",
    "\n",
    "        for obs in self.obstacles_2:\n",
    "            self.board[obs] = -50\n",
    "\n",
    "        for obs in self.obstacles_3:\n",
    "            self.board[obs] = 20\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        state = np.zeros((4, self.board_size, self.board_size), dtype=int)\n",
    "        state[0][self.p1_pos] = 1\n",
    "\n",
    "        for obs in self.obstacles_1:\n",
    "            y, x = obs\n",
    "            state[1][y][x] = 1\n",
    "\n",
    "        for obs in self.obstacles_2:\n",
    "            y, x = obs\n",
    "            state[2][y][x] = 1\n",
    "\n",
    "        for obs in self.obstacles_3:\n",
    "            y, x = obs\n",
    "            state[3][y][x] = 1\n",
    "\n",
    "        return state\n",
    "\n",
    "    def reset_turns(self):\n",
    "        self.numbre_de_tour = 0\n",
    "\n",
    "    def step(self, player, action):\n",
    "        new_pos = self.move(self.p1_pos, action)\n",
    "\n",
    "        if new_pos in self.obstacles_1:\n",
    "            self.p1_pos = self.p1_pos\n",
    "            mur = 'Positif'\n",
    "        else:\n",
    "            self.p1_pos = new_pos\n",
    "            mur = 'RAF'\n",
    "\n",
    "        self.numbre_de_tour += 1  \n",
    "        reward, done = self.get_reward(player, mur)\n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "\n",
    "\n",
    "    def move(self, pos, action):\n",
    "        y, x = pos\n",
    "        if action == 0:  # Up\n",
    "            y = max(0, y - 1)\n",
    "        elif action == 1:  # Down\n",
    "            y = min(self.board_size - 1, y + 1)\n",
    "        elif action == 2:  # Left\n",
    "            x = max(0, x - 1)\n",
    "        elif action == 3:  # Right\n",
    "            x = min(self.board_size - 1, x + 1)\n",
    "        new_pos = (y, x)\n",
    "        return new_pos\n",
    "\n",
    "    \n",
    "    def render(self):\n",
    "        print(\"\\n\")\n",
    "        for i in range(self.board_size):\n",
    "            row = \"\"\n",
    "            for j in range(self.board_size):\n",
    "                if (i, j) == self.p1_pos:\n",
    "                    row += \"P1 \"\n",
    "\n",
    "                elif (i, j) in self.obstacles_1:\n",
    "                    row += \"O \"\n",
    "                elif (i, j) in self.obstacles_2:\n",
    "                    row += \"X \"\n",
    "\n",
    "                elif (i, j) in self.obstacles_3:\n",
    "                    row += \"A \"\n",
    "                else:\n",
    "                    row += \". \"\n",
    "            print(row)\n",
    "\n",
    "    def get_reward(self, player, mur):\n",
    "        reward, done = 0, False\n",
    "\n",
    "        if self.p1_pos == (9, 9):\n",
    "            reward, done = 100, True\n",
    "        elif mur == 'Positif':\n",
    "            reward = -1\n",
    "        elif self.p1_pos in self.obstacles_2:\n",
    "            self.p1_pos = (0, 0)\n",
    "            reward, done = -50, True\n",
    "        elif self.p1_pos in self.obstacles_3:\n",
    "            self.p1_pos = (8, 9)\n",
    "            reward = 10\n",
    "        else:\n",
    "            reward = 0.0\n",
    "\n",
    "        if self.numbre_de_tour >= self.max_steps:  \n",
    "            reward = -100 \n",
    "            done = True  \n",
    "        return reward, done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfa9941",
   "metadata": {},
   "source": [
    "### b) DQN Class\n",
    "\n",
    "Let's create the DQN class defines the architecture of the neural network.\n",
    "As we said above, the Deep Q-Network  is a neural network that approximates the Q-function in Q-learning.The Q-function represents the expected future reward of taking a specific action in a given state. The DQN takes the current state as input and outputs Q-values for all possible actions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6266ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.fc1 = nn.Linear(128 * 10 * 10, 512)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(256, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 10 * 10)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35daa31",
   "metadata": {},
   "source": [
    "### c) Agent class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8130a34",
   "metadata": {},
   "source": [
    "The Agent class handles the learning process of the RL model. It includes the DQN model, the target model, the optimizer, the memory for experience replay, and the action selection.\n",
    "\n",
    "The agent uses the ε-greedy strategy for action selection, which allows it to balance exploration and exploitation. It also uses experience replay and a target network to stabilize learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af19f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, player_id):\n",
    "        self.player_id = player_id\n",
    "        self.model = DQN()\n",
    "        self.target_model = DQN()\n",
    "        self.update_target_model()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001)\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.batch_size = 4\n",
    "        self.gamma = 0.98\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.02\n",
    "        self.save_path = f\"model_player_{player_id}.pth\"\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.randint(0, 3)\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        q_values = self.model(state_tensor)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            next_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "            target = self.model(state_tensor)\n",
    "\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                target[0][action] = reward + self.gamma * torch.max(self.target_model(next_state_tensor))\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = nn.MSELoss()(self.model(state_tensor), target)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def save(self):\n",
    "        if file_name is None:\n",
    "            file_name = self.save_path\n",
    "        torch.save(self.model.state_dict(), file_name)\n",
    "\n",
    "    def load(self):\n",
    "        self.model.load_state_dict(torch.load(self.save_path))\n",
    "        self.update_target_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f4c910",
   "metadata": {},
   "source": [
    "### d) Training Function\n",
    "\n",
    "The train_agent function trains the agent using the custom game environment. It plays multiple games (epochs) and adjusts the agent's behavior based on the experience gained during each game. The agent's performance is evaluated using the total reward and the mean reward of the last games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851fdecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(epochs=1000):\n",
    "    env = GameEnv()\n",
    "    agent = Agent(player_id=1)\n",
    "    reward_history = []\n",
    "\n",
    "    for e in range(epochs):\n",
    "        total_reward = 0\n",
    "        state = env.reset()\n",
    "        env.reset_turns()\n",
    "        print(f\"Starting epoch {e+1}/{epochs}\")\n",
    "        done = False\n",
    "        step_counter = 0 # Compteur d'étapes pour chaque épisode\n",
    "        rec =[]\n",
    "        while not done:\n",
    "            \n",
    "            step_counter += 1\n",
    "            \n",
    "\n",
    "            \n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step(player=1, action=action)\n",
    "            \n",
    "            agent.memorize(state, action, reward, next_state, done)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            rec.append(reward)\n",
    "            \n",
    "            if step_counter % 100 == 0:\n",
    "                \n",
    "                print(f\"Epoch {e+1}/{epochs}, Step {step_counter}\") # Ajouter un message pour indiquer une nouvelle étape\n",
    "                print(\"Raward totale:\", total_reward)\n",
    "                \n",
    "\n",
    "            agent.replay()\n",
    "\n",
    "        reward_history.append(total_reward)\n",
    "\n",
    "        mean_reward = np.mean(reward_history[-100:])\n",
    "            \n",
    "        print(f\"Epoch {e+1}/{epochs} - Total reward for player : {total_reward}\")\n",
    "        print(f\"Epoch {e+1}/{epochs} - Mean reward for player : {mean_reward}\")\n",
    "\n",
    "       \n",
    "        \n",
    "\n",
    "        if e % 5 == 0:\n",
    "            agent.update_target_model()\n",
    "            \n",
    "        if e % 10 == 0:\n",
    "            torch.save(agent.model.state_dict(), \"model_custom_name.pth\")\n",
    "            \n",
    "    # À la fin de la fonction train_agent()\n",
    "    torch.save(agent.model.state_dict(), \"model_custom_name.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6c3c79",
   "metadata": {},
   "source": [
    "### e)  Testing  Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d607dc",
   "metadata": {},
   "source": [
    "After training the agent, you can test its performance using the test function. The function plays a single game using the trained agent and reports the total reward and the number of steps taken to reach the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501fe166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env, agent):\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    step_counter = 0\n",
    "    while not done:\n",
    "        step_counter += 1\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done = env.step(player=1, action=action)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "    print(f\"Total reward during test: {total_reward}\")\n",
    "    print(f\"Steps taken during test: {step_counter}\")\n",
    "    return total_reward, step_counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6238cfd",
   "metadata": {},
   "source": [
    "### f) Run "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cb90ab",
   "metadata": {},
   "source": [
    "- Run the rl_train_script.py to train you first agent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Q-Learn",
   "language": "python",
   "name": "qlearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
